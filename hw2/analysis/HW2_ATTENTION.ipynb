{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11d3b8ab0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import operator\n",
    "from statistics import pstdev\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import sys\n",
    "sys.path.insert(0, \"../code\")\n",
    "from get_result import show_result\n",
    "from WAC import WAC\n",
    "from WAC_ATT import WAC_ATT\n",
    "from WAC_ATT_2 import WAC_ATT_2\n",
    "from WAC_SATT import WAC_SATT\n",
    "from WAC_MATT import WAC_MATT\n",
    "from WAC_MATT_T import WAC_MATT_T\n",
    "from data_pre import data_preprocessing\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime: 1.484684944152832\n"
     ]
    }
   ],
   "source": [
    "(voc_ix, train_data,test_data, dev_data) = data_preprocessing()\n",
    "ix_voc = {}\n",
    "for k,i in voc_ix.items():\n",
    "    ix_voc[i] = k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Averaging Binary Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "* Implemented in `WAC.py` and `wac_eval.py`\n",
    "\n",
    "* I use batch size of 500\n",
    "\n",
    "* Use Adagrad, with `lr = 1e-02`\n",
    "\n",
    "* Use embedding dimension of 100\n",
    "\n",
    "* Use sparse wordembedding with dimension 100; initialized uniformly between -0.l and 0.1 (did find significant improvement compared with when weights initialized between -1 and 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show result for wac.pt\n",
      "performance of best model:\n",
      "model acc on train data: 0.9455968165822803\n",
      "model acc on dev data: 0.8211009174311926\n",
      "model acc on test data: 0.8110928061504667\n"
     ]
    }
   ],
   "source": [
    "name = 'wac.pt'\n",
    "model1 = show_result(name, train_data, dev_data,test_data,sample = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare L2 norm of word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_norm = (model1.word_embeddings.weight.data**2).sum(dim = 1).numpy()\n",
    "word_norm = np.sqrt(word_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEICAYAAABWJCMKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFlZJREFUeJzt3XuUZWV95vHvI+AVVBw6TAONTUxrxKwRnRaY0SgGw1XFrKwozIgtcabNGjA6MkPwkmA0ZMhFzTAKGZQOeAmEGXXZCqO2SgQmIjQOctXYC0G6behGlJvGgP7mj/MWHouqrlPd1XWKfr+ftc6qfd79nvf89u7q85x3731OpaqQJPXnMeMuQJI0HgaAJHXKAJCkThkAktQpA0CSOmUASFKnDIDOJPnrJH84R2Ptm+T+JDu1+3+f5D/MxdhtvP+TZMVcjTeL5/2TJHcluWO+n3s+JXlWkmuT3Jfk9xdAPa9PcsW46+iJAbADSXJrkh+3/9A/TPIPSX4vycP/zlX1e1X1nhHHetmW+lTVd6tq16r66RzU/q4kH5s0/pFVdf62jj3LOvYFTgb2r6p/OcX6Q5JUkrMmtV+R5PXzVOZcOQW4tKp2q6ozh1ckOS7JzZPa1kzTduo81KrtwADY8byiqnYDng6cAfwBcO5cP0mSned6zAViX+D7VbVpC30eAI5PsnRbn2zM+/HpwI3TrLsM+NUki+DhOp8LPGFS279pfWdlYtao8TIAdlBVdU9VrQZeA6xI8msASc5L8idteY8kn22zhbuTXJ7kMUk+yuCF8DPtEM8pSZa2d75vSPJd4MtDbcMvYs9IclWSe5N8OsnT2nMdkmT9cI0Ts4wkRwBvB17Tnu8bbf3Dh5RaXe9McluSTUk+kuQpbd1EHSuSfLcdvnnHdPsmyVPa4ze38d7Zxn8ZsAbYq9Vx3jRD/BA4DzhtmvFHqXWq/XhCktuT/KDN3F6Q5Lr27/OBofF/JclXktzTtvXvtrCtr0xyYxvj75M8u7V/GXgp8IG2rc8cflxVbQBuAV7cmp7PICy+MqntMcDVbcxnt+f4YXvOVw7VcV6Ss5NckuQB4KVJ/kWS1e135SrgGUP9k+T9bf/dm+T6id9hzR0DYAdXVVcB64Ffn2L1yW3dImBPBi/CVVXHA99lMJvYtar+fOgxLwGeDRw+zVO+DvhdYDHwEHDmNP2Ga/wc8KfA37Xne+4U3V7fbi8FfhnYFfjApD4vAp4FHAr80cSL3RT+B/CUNs5LWs0nVNUXgSOB77U6Xr+Fsk8HfjvJs7ay1qn240HAMgah/VfAO4CXAc8BXp3kJa3fe4AvALsD+7TteYT2on4B8BYG/8aXMAj1x1bVbwCXAye1bf3HKYa4jJ+/2L+49b9iUtuVVfVgkl2Az7S6fgl4E/DxSfvn3zHYb7u1cT4I/BOD35XfbbcJh7Xxn8ng3+rVwPen2k5tPQOgD98DnjZF+4MM/vM9vaoerKrLa+Yvh3pXVT1QVT+eZv1Hq+qGqnoA+EMGL1xzMd3/98D7quqWqrofeBtw7KTZxx9X1Y+r6hvANxgcsvgFrZZjgbdV1X1VdSvwXuD42RRTVXcAfw28eytrnWo/vqeq/qmqvsDgMNMFVbWpvRu/HHhe6/cgg8M3e7X+0504fQ1wcVWtqaoHgb8EngD82xE3c/jd/q+3Gi6f1PaVtnwwg6A7o6r+uaq+DHwWOG5ovE9X1f+tqp+1bfht4I/afrgBGD7f8yCDoPhVIFV1c1VtHLFujcgA6MPewN1TtP8FsA74QpJbMtrJvNtnsf42YBdgj5Gq3LK92njDY+/MYOYyYfiqnR8xeEGabI9W0+Sx9t6Kmv4MODzJ5KAZpdap9uOdQ8s/nuL+xPacAgS4qh1qGX7nPG0d7YX3dkbf1suAf5VkdwYv8F+tqm8Ci1vbi/j58f+9gNvbc0yYvF+Ht3kRg30y+fdlotYvM5g1fRDYlOScJE8esW6NyADYwSV5AYP/hI94l9jeAZ9cVb8MvBJ4a5JDJ1ZPM+RMM4QlQ8v7MngndxeDd7RPHKprJwYvAqOO+z0G73qHx36IX3yRHMVd/Pwd9PBYG2Y5DlX1fQaHaiZfVTVKrVv9NbxVdUdV/ceq2gt4I3BWkl+Zousv1JEkDP59RtrWqrqljbES+G6bzQB8tbXtClw59FxLMnTFGY/cr8PbvJnBPpn8+zL8/GdW1b8G9mdwKOi/jlK3RmcA7KCSPDnJy4ELgY9V1fVT9Hl5O6EY4B7gp8DEO7g7GRy/nq3XJtk/yRMZHB753+0y0X8EHp/k6Ha8+J3A44YedyewdNILyLALgP+cZL8ku/LzcwYPzaa4VstFwOlJdkvydOCtwMe2/MhpvY/BIZXh8w1zUut0kvxOkn3a3R8weGH92RRdLwKOTnJo2+cnAz8B/mEWT3c5g/1z+VDbFa1t7dAhrK8xmHWdkmSXJIcAr2Dw+/cI7d/hk8C7kjwxyf7Aw5/5aCfAD2p1P8DgXMFU26htYADseD6T5D4GU+t3MHiBOmGavsuALwL3M3hXd1ZVXdrW/Tfgne2Kjv8yi+f/KIMrZO4AHg/8PgyuSgL+E/BhBu8KH2BwAnrC/2o/v5/k61OMu6qNfRnwHQYvCG+aRV3D3tSe/xYGL2Z/28aftaq6F/hzfvEcy1zWOpUXAF9Lcj+wGnhze7c+ubZvAa9lcJL4LgYvyK+oqn+exXN9hcFJ3eEZ5OWt7eHLP9uYr2BwEv0u4Czgde2Q0XROYjCLuIPB78zfDK17MvAhBgF3G4MTwH8xi7o1gvgHYSSpT84AJKlTBoAkdcoAkKROGQCS1KkF/YVee+yxRy1dunTcZUjSo8o111xzV1Utmqnfgg6ApUuXsnbt2nGXIUmPKklum7mXh4AkqVsGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTC/qTwNvL0lMvfnj51jOOHmMlkjQ+zgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktSpGQMgyZIklya5KcmNSd7c2t+VZEOSa9vtqKHHvC3JuiTfSnL4UPsRrW1dklO3zyZJkkax8wh9HgJOrqqvJ9kNuCbJmrbu/VX1l8Odk+wPHAs8B9gL+GKSZ7bVHwR+E1gPXJ1kdVXdNBcbIkmanRkDoKo2Ahvb8n1Jbgb23sJDjgEurKqfAN9Jsg44sK1bV1W3ACS5sPU1ACRpDGZ1DiDJUuB5wNda00lJrkuyKsnurW1v4Pahh61vbdO1T36OlUnWJlm7efPm2ZQnSZqFkQMgya7AJ4C3VNW9wNnAM4ADGMwQ3jsXBVXVOVW1vKqWL1q0aC6GlCRNYZRzACTZhcGL/8er6pMAVXXn0PoPAZ9tdzcAS4Yevk9rYwvtkqR5NspVQAHOBW6uqvcNtS8e6vZbwA1teTVwbJLHJdkPWAZcBVwNLEuyX5LHMjhRvHpuNkOSNFujzABeCBwPXJ/k2tb2duC4JAcABdwKvBGgqm5MchGDk7sPASdW1U8BkpwEfB7YCVhVVTfO4bZIkmZhlKuArgAyxapLtvCY04HTp2i/ZEuPkyTNHz8JLEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1asYASLIkyaVJbkpyY5I3t/anJVmT5Nvt5+6tPUnOTLIuyXVJnj801orW/9tJVmy/zZIkzWSUGcBDwMlVtT9wMHBikv2BU4EvVdUy4EvtPsCRwLJ2WwmcDYPAAE4DDgIOBE6bCA1J0vybMQCqamNVfb0t3wfcDOwNHAOc37qdD7yqLR8DfKQGrgSemmQxcDiwpqrurqofAGuAI+Z0ayRJI5vVOYAkS4HnAV8D9qyqjW3VHcCebXlv4Pahh61vbdO1S5LGYOQASLIr8AngLVV17/C6qiqg5qKgJCuTrE2ydvPmzXMxpCRpCiMFQJJdGLz4f7yqPtma72yHdmg/N7X2DcCSoYfv09qma/8FVXVOVS2vquWLFi2azbZIkmZhlKuAApwL3FxV7xtatRqYuJJnBfDpofbXtauBDgbuaYeKPg8clmT3dvL3sNYmSRqDnUfo80LgeOD6JNe2trcDZwAXJXkDcBvw6rbuEuAoYB3wI+AEgKq6O8l7gKtbv3dX1d1zshWSpFmbMQCq6gog06w+dIr+BZw4zVirgFWzKVCStH34SWBJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1asYASLIqyaYkNwy1vSvJhiTXtttRQ+velmRdkm8lOXyo/YjWti7JqXO/KZKk2RhlBnAecMQU7e+vqgPa7RKAJPsDxwLPaY85K8lOSXYCPggcCewPHNf6SpLGZOeZOlTVZUmWjjjeMcCFVfUT4DtJ1gEHtnXrquoWgCQXtr43zbpiSdKc2JZzACclua4dItq9te0N3D7UZ31rm679EZKsTLI2ydrNmzdvQ3mSpC3Z2gA4G3gGcACwEXjvXBVUVedU1fKqWr5o0aK5GlaSNMmMh4CmUlV3Tiwn+RDw2XZ3A7BkqOs+rY0ttEuSxmCrZgBJFg/d/S1g4gqh1cCxSR6XZD9gGXAVcDWwLMl+SR7L4ETx6q0vW5K0rWacASS5ADgE2CPJeuA04JAkBwAF3Aq8EaCqbkxyEYOTuw8BJ1bVT9s4JwGfB3YCVlXVjXO+NZKkkY1yFdBxUzSfu4X+pwOnT9F+CXDJrKqTJG03fhJYkjplAEhSp7bqKqAdydJTL354+dYzjh5jJZI0v5wBSFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqVDd/EnL4Tz9KkpwBSFK3DABJ6pQBIEmdMgAkqVMGgCR1asYASLIqyaYkNwy1PS3JmiTfbj93b+1JcmaSdUmuS/L8ocesaP2/nWTF9tkcSdKoRpkBnAccMantVOBLVbUM+FK7D3AksKzdVgJnwyAwgNOAg4ADgdMmQkOSNB4zBkBVXQbcPan5GOD8tnw+8Kqh9o/UwJXAU5MsBg4H1lTV3VX1A2ANjwwVSdI82tpzAHtW1ca2fAewZ1veG7h9qN/61jZduyRpTLb5JHBVFVBzUAsASVYmWZtk7ebNm+dqWEnSJFsbAHe2Qzu0n5ta+wZgyVC/fVrbdO2PUFXnVNXyqlq+aNGirSxPkjSTrQ2A1cDElTwrgE8Ptb+uXQ10MHBPO1T0eeCwJLu3k7+HtTZJ0pjM+GVwSS4ADgH2SLKewdU8ZwAXJXkDcBvw6tb9EuAoYB3wI+AEgKq6O8l7gKtbv3dX1eQTy5KkeTRjAFTVcdOsOnSKvgWcOM04q4BVs6pOkrTd+ElgSeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUzN+F1BPlp568cPLt55x9BgrkaTtzxmAJHXKAJCkThkAktQpA0CSOmUASFKnvApoGl4RJGlH5wxAkjplAEhSpwwASeqU5wC2gecJJD2aOQOQpE4ZAJLUKQ8BbWceJpK0UBkAszT8gj5KuyQtVAbACHxxl7QjMgDGxENDksZtm04CJ7k1yfVJrk2ytrU9LcmaJN9uP3dv7UlyZpJ1Sa5L8vy52ABJ0taZixnAS6vqrqH7pwJfqqozkpza7v8BcCSwrN0OAs5uP7sxyvkDZwOS5sv2uAz0GOD8tnw+8Kqh9o/UwJXAU5Ms3g7PL0kawbbOAAr4QpIC/mdVnQPsWVUb2/o7gD3b8t7A7UOPXd/aNg61kWQlsBJg33333cbyHt2cGUjanrY1AF5UVRuS/BKwJsk3h1dWVbVwGFkLkXMAli9fPqvHSpJGt00BUFUb2s9NST4FHAjcmWRxVW1sh3g2te4bgCVDD9+ntWmIl5xKmi9bfQ4gyZOS7DaxDBwG3ACsBla0biuAT7fl1cDr2tVABwP3DB0qkiTNs22ZAewJfCrJxDh/W1WfS3I1cFGSNwC3Aa9u/S8BjgLWAT8CTtiG55YkbaOtDoCqugV47hTt3wcOnaK9gBO39vl6N+qhIU8WSxqVnwTewXjlkKRR+XXQktQpA0CSOmUASFKnDABJ6pQngTsx3VVEniiW+uUMQJI6ZQBIUqc8BLQDG+XDY35uQOqXMwBJ6pQzAE3JmYG04zMA9DD/ZKXUFw8BSVKnnAFoqzkzkB7dnAFIUqecAWhW/JOV0o7DANCc8HCQ9OjjISBJ6pQBIEmd8hCQ5pzfPCo9OhgAmjeeJ5AWFgNAY2EYSONnAGjsDANpPAwALSieP5DmjwGgRwVnCdLcMwD0qDPqp5ENCmnLDADtsJw1SFu2QweA31ujCZ5bkB5p3gMgyRHAfwd2Aj5cVWfMdw3SVEZ5w2BgaEcyrwGQZCfgg8BvAuuBq5Osrqqb5rMOacJsZ4mzDYm5Ogzl4SxtD/M9AzgQWFdVtwAkuRA4BjAAtMMY5U9rbo/xtyZ4DJa+zXcA7A3cPnR/PXDQcIckK4GV7e79Sb41T7XNhz2Au8ZdxALlvpnayPslfza79q3tt0D4+zK1if3y9FE6L7iTwFV1DnDOuOvYHpKsrarl465jIXLfTM39MjX3y9Rmu1/m++ugNwBLhu7v09okSfNsvgPgamBZkv2SPBY4Flg9zzVIkpjnQ0BV9VCSk4DPM7gMdFVV3TifNYzZDnloa464b6bmfpma+2Vqs9ovqartVYgkaQHzT0JKUqcMAEnqlAEwD5KsSrIpyQ3jrmUhSbIkyaVJbkpyY5I3j7umhSLJ45NcleQbbd/88bhrWiiS7JTk/yX57LhrWUiS3Jrk+iTXJlk70mM8B7D9JXkxcD/wkar6tXHXs1AkWQwsrqqvJ9kNuAZ4lV8NAkkCPKmq7k+yC3AF8OaqunLMpY1dkrcCy4EnV9XLx13PQpHkVmB5VY38ATlnAPOgqi4D7h53HQtNVW2sqq+35fuAmxl8Wrx7NXB/u7tLu3X/bi3JPsDRwIfHXcuOwADQgpBkKfA84GvjrWThaIc6rgU2AWuqyn0DfwWcAvxs3IUsQAV8Ick17St1ZmQAaOyS7Ap8AnhLVd077noWiqr6aVUdwOAT8wcm6frwYZKXA5uq6ppx17JAvaiqng8cCZzYDj1vkQGgsWrHtz8BfLyqPjnuehaiqvohcClwxLhrGbMXAq9sx7ovBH4jycfGW9LCUVUb2s9NwKcYfPvyFhkAGpt2ovNc4Oaqet+461lIkixK8tS2/AQGf0Pjm+Otaryq6m1VtU9VLWXwNTJfrqrXjrmsBSHJk9qFFCR5EnAYMONVhwbAPEhyAfBV4FlJ1id5w7hrWiBeCBzP4J3cte121LiLWiAWA5cmuY7Bd2itqSove9R09gSuSPIN4Crg4qr63EwP8jJQSeqUMwBJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjr1/wEGCa16EhR/jQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(word_norm,bins = 100)\n",
    "plt.title(\"Distribution of Norms of Words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_norm_ix = word_norm.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words with least norm\n",
      "\n",
      "word \t: norm\n",
      "\n",
      "Carrera\t: 0.47581183910369873\n",
      "\n",
      "deathbed\t: 0.4863074719905853\n",
      "\n",
      "waltzes\t: 0.48984774947166443\n",
      "\n",
      "bio\t: 0.4902549684047699\n",
      "\n",
      "Strap\t: 0.4966864585876465\n",
      "\n",
      "Pabst\t: 0.4998193383216858\n",
      "\n",
      "driving\t: 0.5006687045097351\n",
      "\n",
      "Ferzan\t: 0.5024561882019043\n",
      "\n",
      "hedonist\t: 0.5024824738502502\n",
      "\n",
      "sea\t: 0.5034908056259155\n",
      "\n",
      "locales\t: 0.5041689872741699\n",
      "\n",
      "marvels\t: 0.5061100721359253\n",
      "\n",
      "Shakesperean\t: 0.5062376260757446\n",
      "\n",
      "gargantuan\t: 0.5063695907592773\n",
      "\n",
      "unlike\t: 0.5065687894821167\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_top = 15\n",
    "print(\"words with least norm\\n\")\n",
    "print(\"word \\t: norm\\n\")\n",
    "for ix in word_norm_ix[0:n_top]:\n",
    "    print(\"{}\\t: {}\\n\".format(ix_voc[ix], word_norm[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words with greatest norm\n",
      "\n",
      "word \t: norm\n",
      "\n",
      "remarkable\t: 4.497736930847168\n",
      "\n",
      "lacking\t: 4.360334396362305\n",
      "\n",
      "mess\t: 4.239830017089844\n",
      "\n",
      "wonderful\t: 4.0834059715271\n",
      "\n",
      "powerful\t: 4.034727096557617\n",
      "\n",
      "suffers\t: 4.002315998077393\n",
      "\n",
      "devoid\t: 4.000951766967773\n",
      "\n",
      "waste\t: 3.982226848602295\n",
      "\n",
      "hilarious\t: 3.9694108963012695\n",
      "\n",
      "lacks\t: 3.8930931091308594\n",
      "\n",
      "captures\t: 3.7990055084228516\n",
      "\n",
      "flat\t: 3.7967562675476074\n",
      "\n",
      "terrific\t: 3.767505407333374\n",
      "\n",
      "pointless\t: 3.6902318000793457\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_top = 15\n",
    "print(\"words with greatest norm\\n\")\n",
    "print(\"word \\t: norm\\n\")\n",
    "for ix in reversed(word_norm_ix[-n_top:-1]):\n",
    "    print(\"{}\\t: {}\\n\".format(ix_voc[ix], word_norm[ix]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment:\n",
    "\n",
    "* The words with the least norm are those that \n",
    "    * do not have obvious sentiments\n",
    "    * probably rare, even never appear in training\n",
    "\n",
    "* The words with greatest norm are those that:\n",
    "    * have rather clear sentiments\n",
    "    * probably quite frequently appear in training\n",
    "    \n",
    "* If we also take a look at the overall distribution of norms for words, we can see most words haev small weight, thus are not very important for making decision; very few words have big norms as words listed above. Therefore, those strong words are probably strong signal for sentiment analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Attention-Weighted Word Averaging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "* Implemented in `WAC_ATT.py` and `wac_att_eval.py`\n",
    "\n",
    "* I use batch size of 500\n",
    "\n",
    "* Use Adagrad, with `lr = 1e-02`\n",
    "\n",
    "* Use embedding dimension of 100\n",
    "\n",
    "* Use wordembedding with dimension 100; initialized uniformly between -0.l and 0.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis:  Word Embeddings and the Attention Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show result for wac_att.pt\n",
      "performance of best model:\n",
      "model acc on train data: 0.9342974654412092\n",
      "model acc on dev data: 0.8245412844036697\n",
      "model acc on test data: 0.814387699066447\n"
     ]
    }
   ],
   "source": [
    "name = 'wac_att.pt'\n",
    "model2 = show_result(name, train_data, dev_data,test_data,sample = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cos_sim = torch.matmul(model2.u.data, model2.word_embeddings.weight.data.t())\n",
    "#cos_sim = cos_sim.numpy()[0,:]\n",
    "plt.hist(cos_sim.numpy(), bins = 100)\n",
    "plt.title(\"Distribution of cosine similarity of words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_ix = cos_sim.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words with greatest cosine similarity\n",
      "\n",
      "word \t: cosine similarity\n",
      "\n",
      "mess\t: 13.902626037597656\n",
      "\n",
      "bad\t: 13.772995948791504\n",
      "\n",
      "devoid\t: 13.637480735778809\n",
      "\n",
      "waste\t: 13.105344772338867\n",
      "\n",
      "worse\t: 12.94054126739502\n",
      "\n",
      "flat\t: 12.800941467285156\n",
      "\n",
      "lacking\t: 12.536871910095215\n",
      "\n",
      "lacks\t: 12.505407333374023\n",
      "\n",
      "lack\t: 12.405756950378418\n",
      "\n",
      "none\t: 12.111207962036133\n",
      "\n",
      "suffers\t: 11.976886749267578\n",
      "\n",
      "poorly\t: 11.917226791381836\n",
      "\n",
      "pointless\t: 11.910005569458008\n",
      "\n",
      "neither\t: 11.699939727783203\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_top = 15\n",
    "print(\"words with greatest cosine similarity\\n\")\n",
    "print(\"word \\t: cosine similarity\\n\")\n",
    "for ix in reversed(cos_sim_ix[-n_top:-1]):\n",
    "    ix = ix.item()\n",
    "    print(\"{}\\t: {}\\n\".format(ix_voc[ix], cos_sim[ix]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words with smallest cosine similarity\n",
      "\n",
      "word \t: cosine similarity\n",
      "\n",
      "and\t: -11.76754379272461\n",
      "\n",
      "with\t: -11.178474426269531\n",
      "\n",
      "you\t: -10.505175590515137\n",
      "\n",
      "film\t: -10.488924980163574\n",
      "\n",
      "an\t: -10.26484489440918\n",
      "\n",
      "comedy\t: -10.084554672241211\n",
      "\n",
      "one\t: -9.961816787719727\n",
      "\n",
      "a\t: -9.891201972961426\n",
      "\n",
      "has\t: -9.838922500610352\n",
      "\n",
      "makes\t: -9.613327026367188\n",
      "\n",
      "work\t: -9.58224868774414\n",
      "\n",
      "story\t: -9.498851776123047\n",
      "\n",
      ",\t: -9.405157089233398\n",
      "\n",
      "will\t: -9.348079681396484\n",
      "\n",
      "art\t: -9.159615516662598\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_top = 15\n",
    "print(\"words with smallest cosine similarity\\n\")\n",
    "print(\"word \\t: cosine similarity\\n\")\n",
    "for ix in cos_sim_ix[0:n_top:]:\n",
    "    ix = ix.item()\n",
    "    print(\"{}\\t: {}\\n\".format(ix_voc[ix], cos_sim[ix]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment:\n",
    "My finding:\n",
    "* Words that frequently appear but not indicative of sentiments have low cosine similarity, and thus low attention \n",
    "\n",
    "* Words that have obvious, and mostly negative sentiment signal, and less frequent,  has high cosine similarity, and thus higher attention.  \n",
    "\n",
    "Hypothesis:\n",
    "* For the frequent words that don't have clear emotional meanings (appear often in both positive and negative sentences), the model learns to ignore them with small attention. \n",
    "\n",
    "* For the words that appear less often but ofen appear in negative (positive sentence), the model learns to pay more attention to it. \n",
    "\n",
    "* As to why the words that attract most attention are almost all negative, maybe it is related to how people express their feelings: positive word appears more often; a sentence can have some positive word but a single strong negative word  will decide the final sentiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Variance of Attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find freq word (top 100 in training)\n",
    "voc_count = {}\n",
    "for ix in ix_voc.keys():\n",
    "    voc_count[ix] = 0\n",
    "#voc_count[0] = 0\n",
    "for sent in train_data.tensors[0]:\n",
    "    for w in sent.numpy():\n",
    "        if w != 0:\n",
    "            voc_count[w] += 1\n",
    "sorted_voc = sorted(voc_count.items(), key = operator.itemgetter(1))\n",
    "freq_word = [pair[0] for pair in reversed(sorted_voc[-100:-1])]\n",
    "\n",
    "## compute att weights for each frequent word\n",
    "freq_wordset = set(freq_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## copy that dict to a new dict to store (std/mean)\n",
    "X,_,lens = train_data.tensors\n",
    "batch_size, maxlen = X.size()\n",
    "embeds = model2.word_embeddings(X)\n",
    "## build a mask for paddings\n",
    "mask = torch.arange(maxlen)[None,:].float() < lens[:,None].float()\n",
    "#print(mask.size())\n",
    "\n",
    "## compute attention\n",
    "u = torch.unsqueeze(model2.u,0)\n",
    "u = torch.unsqueeze(u,0)\n",
    "#sim = torch.exp(self.cosine(self.u.weight.data.view(1,1,-1), embeds))\n",
    "sim = model2.cosine(u, embeds)\n",
    "sim = torch.mul(sim.exp(), mask.float())\n",
    "\n",
    "att = sim/ sim.sum(dim = 1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RRB-\n",
      "us\n",
      "-LRB-\n",
      "`\n",
      "comedy\n",
      "up\n",
      "enough\n",
      "so\n",
      "characters\n",
      "more\n",
      "out\n",
      "life\n",
      "way\n",
      "much\n",
      "''\n",
      "his\n",
      "'\n",
      "are\n",
      "film\n",
      "something\n",
      "movies\n",
      "movie\n",
      "The\n",
      "can\n",
      "...\n",
      "its\n",
      "story\n",
      "most\n",
      "work\n",
      "and\n"
     ]
    }
   ],
   "source": [
    "att_freqword = {}\n",
    "for w in freq_word:\n",
    "    att_freqword[w] = []\n",
    "    \n",
    "for sent, a,l in zip(X,att, lens):\n",
    "    for w, wa in zip(sent[:l],a[:l]):\n",
    "        if w.item() in freq_wordset:\n",
    "            att_freqword[w.item()].append(wa.item())\n",
    "            \n",
    "att_ratio = {}\n",
    "for w in freq_word:\n",
    "    att_ratio[w] = pstdev(att_freqword[w])/np.mean(att_freqword[w])\n",
    "\n",
    "att_ratio_sort = sorted(att_ratio.items(), key = operator.itemgetter(1))\n",
    "att_ratio_sort = att_ratio_sort[::-1]\n",
    "n_top = 30\n",
    "for (ix,_) in att_ratio_sort[:n_top]:\n",
    "    print(ix_voc[ix])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment:\n",
    "Those words are not clear indicative of emotions themselves. However, their roles in the sentences vary. For example, think about word `film`: sentences `This film ...` can be both positive and negative, and thus `film` in these sentences get small attention' however, `great film` probably only appears in positive review. The model learns (through correlation, since what matters is `good`) that on those occasions, `film` correlates with positive, thus giving it more attention.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-attention\n",
    "\n",
    "\n",
    "## Implementation\n",
    "* Implemented in `WAC_SATT.py` and `WAC_SATTR` respectively. \n",
    "\n",
    "* I use batch size of 500\n",
    "\n",
    "* Use Adagrad, with `lr = 1e-02`\n",
    "\n",
    "* Use embedding dimension of 100\n",
    "\n",
    "* Use wordembedding with dimension 100; initialized uniformly between -0.l and 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show result for wac_satt.pt\n",
      "performance of best model:\n",
      "model acc on train data: 0.9061901438774147\n",
      "model acc on dev data: 0.8130733944954128\n",
      "model acc on test data: 0.7984623833058759\n"
     ]
    }
   ],
   "source": [
    "name = 'wac_satt.pt'\n",
    "model3 = show_result(name, train_data, dev_data,test_data,sample = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show result for wac_sattr.pt\n",
      "performance of best model:\n",
      "model acc on train data: 0.9190336901809975\n",
      "model acc on dev data: 0.8142201834862385\n",
      "model acc on test data: 0.8072487644151565\n"
     ]
    }
   ],
   "source": [
    "name = 'wac_sattr.pt'\n",
    "model4 = show_result(name, train_data, dev_data,test_data,sample = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment\n",
    "The self-attention models are not very stable in training. Residual connection makes it better, but still it is fragile in training with accuracy dropping rapidly after a few epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enriching the Attention Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous 3 models, I find that the best model is model 2 (the model that computes attention by cosine similarity with a vector $u$). So I build on its architecture.  All the other hyperparameters remain the same as previous ones. \n",
    "\n",
    "I begin by separating the key, query and value (call the model `wac_att_2`, implemented in `WAC_ATT_2.py`). Seeing it performs well, I  add another head to it (call it model `wac_matt`, implemented in `WAC_MATT.py`). \n",
    "\n",
    "Then I think $u$ is a global variable for all sentences. This means the absolute attention (similarity score) is fixed for each word, no matter in what context. Maybe we need to let it be senetence-specific. \n",
    "\n",
    "Thus I use LSTM to get a \"thought vector\" (the last hidden vector) which I think should contain context information. Then the final $u$ is the convex combination between this thought vector and original $u$. However, it is very slow to train and the result is not as good as simpler model. (This model is called `wac_attt`, with the last `t` meaning thought vector; implemented in `WAC_ATT_T.py`)\n",
    "\n",
    "I continue with this idea, and use the \"thought vector\" to compute attention for the second head. Again, it is very slow to train and the result is not as good as simpler model.  (This model is called `wac_mattt`, with the last `t` meaning thought vector; implemented in `WAC_MATT_T.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show result for wac_att_2.pt\n",
      "performance of best model:\n",
      "model acc on train data: 0.937\n",
      "model acc on dev data: 0.8211009174311926\n",
      "model acc on test data: 0.8220757825370676\n"
     ]
    }
   ],
   "source": [
    "name = 'wac_att_2.pt'\n",
    "_ = show_result(name, train_data, dev_data,test_data,sample = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show result for wac_matt.pt\n",
      "performance of best model:\n",
      "model acc on train data: 0.936\n",
      "model acc on dev data: 0.8222477064220184\n",
      "model acc on test data: 0.8209774848984075\n"
     ]
    }
   ],
   "source": [
    "name = 'wac_matt.pt'\n",
    "_ = show_result(name, train_data, dev_data,test_data,sample = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show result for wac_attt.pt\n",
      "performance of best model:\n",
      "model acc on train data: 0.932\n",
      "model acc on dev data: 0.8176605504587156\n",
      "model acc on test data: 0.8105436573311368\n"
     ]
    }
   ],
   "source": [
    "name = 'wac_attt.pt'\n",
    "_ = show_result(name, train_data, dev_data,test_data,sample = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show result for wac_mattt.pt\n",
      "performance of best model:\n",
      "model acc on train data: 0.934\n",
      "model acc on dev data: 0.8130733944954128\n",
      "model acc on test data: 0.8077979132344866\n"
     ]
    }
   ],
   "source": [
    "name = 'wac_mattt.pt'\n",
    "_ = show_result(name, train_data, dev_data,test_data,sample = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "### Result:\n",
    "The best model (in test dataset) so far is `wac_att_2`, which is just separating the key, value of the model in problem 2. \n",
    "\n",
    "With multihead (two head), it does better in development data, but slightly worse in test dataset. \n",
    "\n",
    "Although those that use \"thought vector\" are more complex and should learn better in theory, they do not do that well. Maybe I can tune parameter (for example parameter `lam` that controls the convex combination), but the model takes too long to train (120 seconds per epoch, whereas other LSTM-free model takes less than 10 seconds to train). \n",
    "\n",
    "### Discussion:\n",
    "From the accuracy on train, test, dev set, we can see the model overfits. Therefore, we might be able to do better on those complicated models if we add more penalty, or use sparse embedding (for some reason I cannot use sparse embedding for those models). Thus we cannot reach a conclusion about the best architecture on this task yet. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
