{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "1.0.1.post2\n",
      "data preprocessing takes 1.112109899520874\n",
      "finish preparing data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import log\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from WAC import WAC\n",
    "from data_pre import data_preprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)\n",
    "\n",
    "'''\n",
    "TO DO: \n",
    "* Use batching\n",
    "    * set grad to be 0\n",
    "    * set loss to be 0\n",
    "    * loss.backward() and optimizer\n",
    "'''\n",
    "\n",
    "'''\n",
    "load and prepare data\n",
    "'''\n",
    "(voc_ix, trainX, trainy, testX, testy, devX, devy) = data_preprocessing() \n",
    "print(\"finish preparing data\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DataLoader((trainX, trainy), batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-888d493fd8d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67349"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds.dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "set parameters\n",
    "'''          \n",
    "## set hyperparameters \n",
    "VOCAB_SIZE = len(voc_ix)\n",
    "EMBEDDING_DIM = 100\n",
    "eval_per = 5000\n",
    "n_epoch = 5\n",
    "bsize = 500\n",
    "PATH = \"../model/wac.pt\"\n",
    "\n",
    "## define model \n",
    "model = WAC(EMBEDDING_DIM, VOCAB_SIZE)\n",
    "optimizer = optim.Adagrad(model.parameters(), lr = 1e-2, lr_decay = 1e-4)\n",
    "\n",
    "\n",
    "'''\n",
    "training\n",
    "'''\n",
    "## training \n",
    "losses = []\n",
    "accs = []\n",
    "i = 0\n",
    "best_dev_acc = 0\n",
    "start = time.time()\n",
    "for epoch in range(n_epoch):  \n",
    "    print(\"epoch \" + str(epoch))\n",
    "    total_loss = 0\n",
    "    loss = 0\n",
    "\n",
    "    ds = DataLoader((trainX, trainy), batch_size=10, shuffle=True)\n",
    "    #(trainX, trainy) = ds.dataset\n",
    "\n",
    "    for X,y in enumerate(ds):\n",
    "\n",
    "        if i % eval_per == 0:\n",
    "            print(\"time: {}\".format(time.time() - start))\n",
    "            acc = model.evaluate(devX, devy)\n",
    "            if acc > best_dev_acc:\n",
    "                best_dev_acc = acc\n",
    "                torch.save(model, PATH)\n",
    "            print(\"accuracy on dev: \" + str(acc))\n",
    "            accs.append(acc)\n",
    "        \n",
    "        # Step 3. Run our forward pass.\n",
    "        prob = model.forward(X)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss_sent = - y*log(prob) - (1-y)*log(1-prob) \n",
    "        loss += loss_sent\n",
    "\n",
    "        if i % bsize == 0:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "            total_loss += loss.item()\n",
    "            loss = 0\n",
    "        i +=1\n",
    "        \n",
    "    losses.append(total_loss)\n",
    "runtime = time.time() - start\n",
    "print(\"runtime: \" + str(runtime) + \"s\")\n",
    "\n",
    "model_best = torch.load(PATH)\n",
    "model_best.eval()\n",
    "acc_dev = model_best.evaluate(devX, devy)\n",
    "print(\"best model acc on dev: \" + str(acc_dev))\n",
    "acc_test = model_best.evaluate(testX, testy)\n",
    "print(\"best model acc on test: \" + str(acc_test))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
