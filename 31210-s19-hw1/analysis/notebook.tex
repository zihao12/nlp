
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{hw1\_report}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
        \PY{k+kn}{import} \PY{n+nn}{torch}
        \PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn} \PY{k}{as} \PY{n+nn}{nn}
        \PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn}\PY{n+nn}{.}\PY{n+nn}{functional} \PY{k}{as} \PY{n+nn}{F}
        \PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{optim} \PY{k}{as} \PY{n+nn}{optim}
        \PY{k+kn}{import} \PY{n+nn}{time}
        \PY{k+kn}{import} \PY{n+nn}{sys}
        \PY{n}{sys}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{insert}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{../code}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{k+kn}{from} \PY{n+nn}{LSTMlm} \PY{k}{import} \PY{n}{LSTMlm}
        \PY{k+kn}{from} \PY{n+nn}{LSTMlm\PYZus{}ctx} \PY{k}{import} \PY{n}{LSTMlm\PYZus{}ctx}
        \PY{k+kn}{from} \PY{n+nn}{data\PYZus{}pre} \PY{k}{import} \PY{n}{data\PYZus{}preprocessing}\PY{p}{,}\PY{n}{data\PYZus{}preprocessing\PYZus{}ctx}
        \PY{k+kn}{from} \PY{n+nn}{misc} \PY{k}{import} \PY{n}{common\PYZus{}error\PYZus{}pair}\PY{p}{,}\PY{n}{common\PYZus{}error\PYZus{}pair\PYZus{}ctx}\PY{p}{,} \PY{n}{plot\PYZus{}acc\PYZus{}neg}\PY{p}{,}\PY{n}{plot\PYZus{}acc\PYZus{}negchinge}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{manifold} \PY{k}{import} \PY{n}{TSNE}
        \PY{k+kn}{import} \PY{n+nn}{pickle}
        \PY{k+kn}{import} \PY{n+nn}{warnings}
        \PY{n}{warnings}\PY{o}{.}\PY{n}{filterwarnings}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ignore}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{torch}\PY{o}{.}\PY{n}{manual\PYZus{}seed}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}1}]:} <torch.\_C.Generator at 0x10cfc0b70>
\end{Verbatim}
            
    \section{LSTM language model Trained with Hinge
Loss}\label{lstm-language-model-trained-with-hinge-loss}

    \begin{itemize}
\tightlist
\item
  I implemented using \texttt{../code/lstm\_ll.py} using pytorch; the
  result is at \texttt{../output/lstm\_ll.pyout}. Best model is stored
  at \texttt{../mode/lstmlm\_ll.pt}
\item
  I use LSTM with dimensionality 200 for both word embeddings and the
  LSTM cell/hidden vectors
\item
  I use \texttt{Adam} optimizer with default learning rate
  (\texttt{lr\ =\ 1e-3}).
\item
  I didn't use mini-batching
\item
  I borrowed code from
  https://pytorch.org/tutorials/beginner/nlp/sequence\_models\_tutorial.html
\item
  For early stopping, I didn't stop the algorithm when accuracy on dev
  drops. Instead I save the best model and let it run and observe its
  behavior.
\end{itemize}

    \section{Error Analysis}\label{error-analysis}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} load data}
        \PY{p}{(}\PY{n}{voc\PYZus{}ix}\PY{p}{,} \PY{n}{data\PYZus{}train}\PY{p}{,} \PY{n}{data\PYZus{}test}\PY{p}{,} \PY{n}{data\PYZus{}dev}\PY{p}{)} \PY{o}{=} \PY{n}{data\PYZus{}preprocessing}\PY{p}{(}\PY{p}{)}
        \PY{n}{ix\PYZus{}voc} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
        \PY{k}{for} \PY{n}{k}\PY{p}{,}\PY{n}{i} \PY{o+ow}{in} \PY{n}{voc\PYZus{}ix}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{n}{ix\PYZus{}voc}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{k}
        \PY{n}{PATH} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{../model/lstmlm\PYZus{}ll.pt}\PY{l+s+s2}{\PYZdq{}}
        \PY{c+c1}{\PYZsh{}\PYZsh{} evaluate data}
        \PY{n}{model1} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{n}{PATH}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{performance of best model:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{model acc on dev data: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{model1}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{data\PYZus{}dev}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{model acc on test data: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{model1}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{data\PYZus{}test}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
performance of best model:
model acc on dev data: 0.3351765740857107
model acc on test data: 0.32559870951730985

    \end{Verbatim}

    \subsubsection{Comment on results:}\label{comment-on-results}

Best accuracy on development set (33 percent) is achieved at the end of
epoch 2 (with around 12000 sentences), and then gradually drops to below
20 percent. So although total running time is 2076 seconds (for two
epochs), we only need to run 200 seconds to get optimal.

    \subsubsection{Error pair analysis}\label{error-pair-analysis}

Implemented at \texttt{code/misc.py}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n}{common\PYZus{}error1} \PY{o}{=} \PY{n}{common\PYZus{}error\PYZus{}pair}\PY{p}{(}\PY{n}{model1}\PY{p}{,} \PY{n}{data\PYZus{}test}\PY{p}{,} \PY{l+m+mi}{35}\PY{p}{,} \PY{n}{ix\PYZus{}voc}\PY{p}{)}
\end{Verbatim}


    \begin{itemize}
\item
  Error categories and abbreviations

\begin{verbatim}
(start of sentence: SOS)
early stop: ES
late stop: LS
wrong person: WP
right person: RP
wrong verb: WV
catch-all: CA
wrong word type: WT
\end{verbatim}
\item
  Labeled Error:
\end{itemize}

\begin{verbatim}
(He Bob): 136; RP SOS
(She Bob): 112; WP SOS
(Sue Bob): 110; WP (SOS in most cases) 
(to .): 58; ES
(and .): 58; ES
(decided was): 47; WV
(had was): 44; WV
(in .): 38; ES
(for .): 37; ES
(his the): 36; CA
(, .): 32; ES
(her the): 30; WP
(His Bob): 27; WT; SOS
(One Bob): 25; RP; SOS
(The Bob): 23; RP; SOS
(the .): 22; ES
(got was): 21; WV
(he Bob): 21; RP
(But Bob): 21; WT, SOS
(Her Bob): 21; WT, WP, SOS
(with .): 20; ES
(went was): 20; WV
('s was): 19; CA
(When Bob): 19; CA
(her a): 19; CA
(a the): 19; CA
(the a): 19;CA
(They Bob): 19; WP
(wanted was): 19; WV
(at .): 18; ES;
(! .): 18; CA
(the her): 17; CA
(it the): 16; WT
(. to): 16; LS
(of .): 16; ES
\end{verbatim}

    \subsubsection{Comment:}\label{comment}

\paragraph{Analysis of error}\label{analysis-of-error}

(Note: For some error pairs I give them two labels.)

\begin{itemize}
\item
  The most frequent error are "start of sentence error". In fact, the
  model almost always predict "Bob" for start of sentence. This is
  because in training most sentences start with "Bob" so the model
  learns that "start of sentence" sign will most likely lead to "Bob".
\item
  Similar to "Bob", "." is one of the most frequent words. So the model
  uses "." appropriately often. It is noteworthy that, there are not
  many misuse of End Of Sentence. Of course it is easy for the model to
  learn that. But that means the model fails to realize that "." always
  means the end of sentence.
\item
  Many error use the wrong verb, typically "decided", "had" was
  predicted as "was". This is probably because "was" simply appeared
  more often, and they appeared at similar places (probably after a
  person, Bob/Sue).so the model gives more probability to it. In this
  sense, the model does well as it assigns the right type of word.
\end{itemize}

\paragraph{How can the model does
better}\label{how-can-the-model-does-better}

As we can see, the most frequent error are wrong start of sentence.
Indeed without previous sentence, \(P(\text{start with Bob})\) is the
highest. But \(P(\text{start with Bob} | \ \text{previous sentence})\)
will probably not be that high.

\paragraph{Visualization}\label{visualization}

I also tried visualizing the word embedding from the model using TSNE.
They are stored at \texttt{../docs/}. But it is not very helpful to find
relationship between error pairs. Next time I will try to find their
nearest neighbor.

    \section{Binary Loss Implementation and
Experimentation}\label{binary-loss-implementation-and-experimentation}

    \subsection{Implementation:}\label{implementation}

\begin{itemize}
\tightlist
\item
  The training is unstable using default Adam learning rate. It is easy
  to have accuracy suddenly drop to 0 with \texttt{nan} in weights
\item
  So I decreased learning rate to \texttt{5*1e-04} and set
  \texttt{eps\ =\ 1e-3}. The latter is added to dividend to increase
  stability. I also clip the norm witn parameter 0.25. (I discussed with
  Young Joo Yun on this issue).
\item
  The code is \texttt{"../code/lstm\_neg.py"}
\item
  The model is saved at \texttt{"../model/lstmlm\_neg\_f\{\}\_r\{\}.pt"}
  with their corresponding (f,r)
\item
  output file are at \texttt{"../output/lstm\_neg\_r\{\}\_f\{\}.pyout"}
  with their corresponding (f,r)
\end{itemize}

    \subsection{UNIF}\label{unif}

\begin{verbatim}
r       f       acc_dev         acc_test        #sent/sec       #sent for max acc       time for max acc
20      0       0.2563          0.2451          42.7            102612                  2403
100     0       0.2609          0.2499          38.2            115684                  3031
500     0       0.2634          0.2505          23.7            109648                  4617
\end{verbatim}

I also estimated those measures for log loss:

\begin{verbatim}
#sent/sec       #sent for max acc       time for max acc
  58.13               12036            200
\end{verbatim}

\subsubsection{Comment:}\label{comment}

\subparagraph{Comparison within negative sampling (the effect of
r)}\label{comparison-within-negative-sampling-the-effect-of-r}

\begin{itemize}
\tightlist
\item
  Smaller \(r\) can be faster to process one sentence (bigger
  \texttt{\#sent/sec})
\item
  It is not clear how \(r\) affact \texttt{\#sent\ for\ max\ acc}. In
  the three experiments above, \(r\) seems not to affact
  \texttt{\#sent\ for\ max\ acc} much. As a result, in this case,
  \texttt{time\ for\ max\ acc} is smaller with bigger
  \texttt{\#sent/sec}.
\item
  Before doing the experiments, I thought choosing \(r\) is a trade-off
  beteen speed per step, and the "progress" made per step; however, from
  the accuracy plot blow, we can see size of \(r\) does not affact
  "progress" that much. It does affect the variance during training:
  with small \(r\), the accuracy trajectory experiences bigger climbing
  and falling.
\end{itemize}

\paragraph{Comparison between log loss and binary log
loss}\label{comparison-between-log-loss-and-binary-log-loss}

\begin{itemize}
\tightlist
\item
  log loss does better in all those measures, and achieve much better
  accuracy.
\item
  \texttt{\#sent/sec} might be strongly affacted by my own
  implementation. If we only compare flop counts, binary log loss with
  negative sampling should be faster in processing one sentence.
\item
  The \texttt{\#sent\ for\ max\ acc} is much smaller than binary log
  loss (\(10^4\) vs \(10^5\))
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{n}{plot\PYZus{}acc\PYZus{}neg}\PY{p}{(}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_13_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n}{plot\PYZus{}acc\PYZus{}neg}\PY{p}{(}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_14_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{plot\PYZus{}acc\PYZus{}neg}\PY{p}{(}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mi}{500}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_15_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{UNIG-f}\label{unig-f}

    First, i tried f from 0.1 to 1 with step size 0.1. The results are as
follow:

\begin{verbatim}
r       f       acc_dev         acc_test        #sent/sec       #sent for max acc       time for max acc
20      0.1     0.2324          0.2206          36.0            30144                   836
20      0.2     0.2052          0.1997          43.7            14072                   321
20      0.3     0.1827          0.1762          41.7            12036                   288
20      0.4     0.1573          0.1493          41.7            19108                   458
20      0.5     0.1355          0.1297          41.6            13072                   314
20      0.6     0.1096          0.1090          46.5            14072                   302
20      0.7     0.1026          0.1006          45.8            16072                   350
20      0.8     0.0944          0.0930          44.6            22180                   495
20      0.9     0.0934          0.0919          45.4            26144                   576
20      1.0     0.0883          0.0858          47.92           13072                   272
\end{verbatim}

\begin{itemize}
\tightlist
\item
  None of them perform better than UNIF
\item
  It is noteworthy that maximum accuracy on dev is achieved in around 1
  or 2 or 3 epochs, whereas in when \(f = 0\) it takes more than 10
  epochs to achieve maximum accuracy.
\item
  Therefore, there might be problem with optimization. Below I show
  accuracy plot of one model
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n}{plot\PYZus{}acc\PYZus{}neg}\PY{p}{(}\PY{n}{f} \PY{o}{=} \PY{l+m+mf}{0.0}\PY{p}{,} \PY{n}{r} \PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_18_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{n}{plot\PYZus{}acc\PYZus{}neg}\PY{p}{(}\PY{n}{f} \PY{o}{=} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{n}{r} \PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{n}{plot\PYZus{}acc\PYZus{}neg}\PY{p}{(}\PY{n}{f} \PY{o}{=} \PY{l+m+mf}{1.0}\PY{p}{,} \PY{n}{r} \PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_20_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Optimization issue:}\label{optimization-issue}

\begin{itemize}
\item
  From the plots above, I can see that when \(f > 0\), there is a
  serious optimization problem in training. When \(f=0\), the problem is
  less severe.
\item
  I have decreased my learning rate to \(5*10^{-4}\) and set
  \(eps = 10^{-3}\) and add \texttt{clip\_norm} to increase stability.
  However, they seem not to be enough to stabilize training.
\item
  If I had more time, I would try \(r = 500\) or even larger and use a
  even smaller learning rate.
\end{itemize}

    \section{Using Context}\label{using-context}

    \begin{itemize}
\tightlist
\item
  I implemented using \texttt{../code/lstm\_ll\_ctx.py} using pytorch;
  the result is at \texttt{../output/lstm\_ll\_ctx.pyout}. Best model is
  stored at \texttt{../mode/lstmlm\_ll\_ctx.pt}
\item
  I use the same model hyperparameters as the first model.
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} load data}
         \PY{p}{(}\PY{n}{voc\PYZus{}ix}\PY{p}{,} \PY{n}{data\PYZus{}train\PYZus{}ctx}\PY{p}{,} \PY{n}{data\PYZus{}test\PYZus{}ctx}\PY{p}{,} \PY{n}{data\PYZus{}dev\PYZus{}ctx}\PY{p}{)} \PY{o}{=} \PY{n}{data\PYZus{}preprocessing\PYZus{}ctx}\PY{p}{(}\PY{p}{)}
         \PY{n}{ix\PYZus{}voc} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
         \PY{k}{for} \PY{n}{k}\PY{p}{,}\PY{n}{i} \PY{o+ow}{in} \PY{n}{voc\PYZus{}ix}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:}
             \PY{n}{ix\PYZus{}voc}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{k}
         
         \PY{n}{PATH} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{../model/lstmlm\PYZus{}ll\PYZus{}ctx.pt}\PY{l+s+s2}{\PYZdq{}}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{} evaluate data}
         \PY{n}{model3} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{n}{PATH}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{performance of best model:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{model acc on dev data: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{model3}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{data\PYZus{}dev\PYZus{}ctx}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{model acc on test data: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{model3}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{data\PYZus{}test\PYZus{}ctx}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
performance of best model:
model acc on dev data: 0.36433329144149806
model acc on test data: 0.3588534557637424

    \end{Verbatim}

    \subsubsection{Error pair analysis}\label{error-pair-analysis}

Implemented at \texttt{code/misc.py}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{n}{common\PYZus{}error2} \PY{o}{=} \PY{n}{common\PYZus{}error\PYZus{}pair\PYZus{}ctx}\PY{p}{(}\PY{n}{model3}\PY{p}{,} \PY{n}{data\PYZus{}test\PYZus{}ctx}\PY{p}{,} \PY{l+m+mi}{35}\PY{p}{,} \PY{n}{ix\PYZus{}voc}\PY{p}{)}
\end{Verbatim}


    \begin{verbatim}
(to .): 68; ES
(and .): 57; ES
(had was): 46; WV
(decided was): 38; WV
(for .): 37; ES
(Bob He): 36; RP; SOS
(, .): 32; ES;
(in .): 29; ES
(his the): 27; CA
(the her): 27;CA
(Bob Sue): 27;RP
(Sue She): 24;RP
(her the): 24; CA
(His He): 22; WT
(Her She): 21; WT
(the .): 21; ES
(went was): 20; WV
(a the): 20; CA
(wanted was): 20; WV
(got was): 18; WV
(! .): 18; ES
(on .): 18; ES
(Sue Bob): 17; WP
(with .): 16; ES
('s was): 16; CA
(at .): 16; ES
(her a): 15; CA
(. to): 15; LS
(the a): 14; CA
(a .): 14; ES
(asked was): 14; WV
(the his): 13; CA
(didn was): 13; CA
(it the): 12; CA
(She Sue): 12; RP
\end{verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comparison between with and without context}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{(yg, yp): number w/ ctx VS number w/o ctx}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{k}{for} \PY{n}{ep1} \PY{o+ow}{in} \PY{n}{common\PYZus{}error1}\PY{p}{:}
             \PY{k}{for} \PY{n}{ep2} \PY{o+ow}{in} \PY{n}{common\PYZus{}error2}\PY{p}{:}
                 \PY{k}{if} \PY{n}{ep1}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{==} \PY{n}{ep2}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{:}
                     \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{(}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{): }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ vs }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{ix\PYZus{}voc}\PY{p}{[}\PY{n}{ep1}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{n}{ix\PYZus{}voc}\PY{p}{[}\PY{n}{ep1}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{n}{ep1}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{ep2}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}
                     \PY{k}{break}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{(}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{): }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ vs \PYZlt{} 16}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{ix\PYZus{}voc}\PY{p}{[}\PY{n}{ep1}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{n}{ix\PYZus{}voc}\PY{p}{[}\PY{n}{ep1}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{n}{ep1}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{)}\PY{p}{)}
                     
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Comparison between with and without context
(yg, yp): number w/ ctx VS number w/o ctx
\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#
(He Bob): 136 vs < 16
(She Bob): 112 vs < 16
(Sue Bob): 110 vs 17
(Sue Bob): 110 vs < 16
(to .): 58 vs 68
(to .): 58 vs < 16
(and .): 58 vs 57
(and .): 58 vs < 16
(decided was): 47 vs 38
(decided was): 47 vs < 16
(had was): 44 vs 46
(had was): 44 vs < 16
(in .): 38 vs 29
(in .): 38 vs < 16
(for .): 37 vs 37
(for .): 37 vs < 16
(his the): 36 vs 27
(his the): 36 vs < 16
(, .): 32 vs 32
(, .): 32 vs < 16
(her the): 30 vs 24
(her the): 30 vs < 16
(His Bob): 27 vs < 16
(One Bob): 25 vs < 16
(The Bob): 23 vs < 16
(the .): 22 vs 21
(the .): 22 vs < 16
(got was): 21 vs 18
(got was): 21 vs < 16
(he Bob): 21 vs < 16
(But Bob): 21 vs < 16
(Her Bob): 21 vs < 16
(with .): 20 vs 16
(with .): 20 vs < 16
(went was): 20 vs 20
(went was): 20 vs < 16
('s was): 19 vs 16
('s was): 19 vs < 16
(When Bob): 19 vs < 16
(her a): 19 vs 15
(her a): 19 vs < 16
(a the): 19 vs 20
(a the): 19 vs < 16
(the a): 19 vs 14
(the a): 19 vs < 16
(They Bob): 19 vs < 16
(wanted was): 19 vs 20
(wanted was): 19 vs < 16
(at .): 18 vs 16
(at .): 18 vs < 16
(! .): 18 vs 18
(! .): 18 vs < 16
(the her): 17 vs 27
(the her): 17 vs < 16
(it the): 16 vs 12
(it the): 16 vs < 16
(. to): 16 vs 15
(. to): 16 vs < 16
(of .): 16 vs < 16

    \end{Verbatim}

    \subsubsection{Comment:}\label{comment}

Many "start of sentence" error are corrected, and when they appeared,
they mostly referred to the same person ("Bob", "He").

Many other error, like the wrong verb used and early stopping sentence
with ".", are not corrected.

    \section{Hinge Loss Implementation}\label{hinge-loss-implementation}

    \subsection{Implementation:}\label{implementation}

\begin{itemize}
\tightlist
\item
  The training is unstable using default Adam learning rate. It is easy
  to have accuracy suddenly drop to 0 with \texttt{nan} in weights
\item
  So I decreased learning rate to \texttt{5*1e-04} and set
  \texttt{eps\ =\ 1e-3}. The latter is added to dividend to increase
  stability. I also clip the norm witn parameter 0.25. (I discussed with
  Young Joo Yun on this issue).
\item
  The code is \texttt{"../code/lstm\_neg\_chinge.py"}
\item
  The model is saved at
  \texttt{"../model/lstm\_neg\_chinge\_f\{\}\_r\{\}.pt"} with their
  corresponding (f,r)
\item
  output file are at
  \texttt{"../output/lstm\_neg\_chinge\_r\{\}\_f\{\}.pyout"} with their
  corresponding (f,r)
\end{itemize}

    \subsection{UNIF}\label{unif}

\begin{verbatim}
r       f       acc_dev         acc_test        #sent/sec       #sent for max acc       time for max acc
20      0       0.2629          0.2474          38.9            99576                   2559
100     0       0.2769          0.2624          36.3            103612                  2855
500     0       0.2761          0.2578          24.1            120684                  5018
\end{verbatim}

I also estimated those measures for log loss:

\begin{verbatim}
#sent/sec       #sent for max acc       time for max acc
  58.13               12036            200
\end{verbatim}

\subsubsection{Comment:}\label{comment}

My conclulsion about the three measure are simil;

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{n}{plot\PYZus{}acc\PYZus{}negchinge}\PY{p}{(}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_33_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{n}{plot\PYZus{}acc\PYZus{}negchinge}\PY{p}{(}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_34_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{n}{plot\PYZus{}acc\PYZus{}negchinge}\PY{p}{(}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mi}{500}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_35_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{UNIG-F}\label{unig-f}

\begin{verbatim}
r       f       acc_dev         acc_test        #sent/sec       #sent for max acc       time for max acc
20      0.1     0.2613          0.2490          44.8            115684                  2584
20      0.2     0.2575          0.2377          43.8            119684                  2733
20      0.3     0.2557          0.2387          45.1            108612                  2410
20      0.4     0.2391          0.2227          46.4            119684                  2581
20      0.5     0.2213          0.2108          45.2            109648                  2423
20      0.6     0.1956          0.1894          45.8            120684                  2636
20      0.7     0.1816          0.1655          43.7            115684                  2647
20      0.8     0.1625          0.1536          46.8            106612                  2276
20      0.9     0.1509          0.1433          45.0            114648                  2546
20      1.0     0.1391          0.1332          45.0            96540                   2145
\end{verbatim}

    \subsubsection{Comment:}\label{comment}

\begin{itemize}
\tightlist
\item
  For \(r=20\), UNIG-F beats UNIF (f = 0.1) in accuracy on test set (the
  advantage is pretty small).
\item
  While in binary log loss, accuracy drops drastically as \(f\) gets
  larger, hinge loss is slightly better. The plot below show training
  manages to get high (compared with binary loss) before it drops
  drastically.
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{n}{plot\PYZus{}acc\PYZus{}negchinge}\PY{p}{(}\PY{n}{f} \PY{o}{=} \PY{l+m+mf}{0.0}\PY{p}{,} \PY{n}{r} \PY{o}{=} \PY{l+m+mi}{20}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_38_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{n}{plot\PYZus{}acc\PYZus{}negchinge}\PY{p}{(}\PY{n}{f} \PY{o}{=} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{n}{r} \PY{o}{=} \PY{l+m+mi}{20}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_39_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{n}{plot\PYZus{}acc\PYZus{}negchinge}\PY{p}{(}\PY{n}{f} \PY{o}{=} \PY{l+m+mf}{0.5}\PY{p}{,} \PY{n}{r} \PY{o}{=} \PY{l+m+mi}{20}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_40_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{n}{plot\PYZus{}acc\PYZus{}negchinge}\PY{p}{(}\PY{n}{f} \PY{o}{=} \PY{l+m+mf}{1.0}\PY{p}{,} \PY{n}{r} \PY{o}{=} \PY{l+m+mi}{20}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_41_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \section{Reflection}\label{reflection}

\subsection{Interesting mistakes}\label{interesting-mistakes}

I made a few mistakes during the experiments, some of which are
meaningful:

\begin{itemize}
\item
  I misused the cross entropy loss function in pytorch. It automatically
  turns score into probability through softmax. Without knowing it, I
  did softmax over sccore and as the probability as input. In effect, my
  model does softmax twice to compute log loss. As a result, my model
  gets 30 percent accuracy both with and without context. Probably
  because double softmax softens the signal too much, the update cannot
  make enough progress. (Also, I have been thinking if there is an
  alternative to softmax that can turn score into probability, which may
  work better.)
\item
  I foolishly set seed for my random sampling,and the accuracy is just
  one percent! This makes sense because it is only trained on those
  small subset of negative samples...
\end{itemize}

\subsection{Remaining issues and future
explorations}\label{remaining-issues-and-future-explorations}

\begin{itemize}
\item
  I think there is still a lot of room for improvement on both binary
  loss and hinge loss by tunning the parameters
\item
  Maybe we can sample negative samples with weights proportional to the
  frequency of error pair. Or maybe we can train with log loss first,
  then switch to that sampling scheme? It may work since it seems to be
  able to force the model to make corrections on previous frequent
  error.
\end{itemize}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
